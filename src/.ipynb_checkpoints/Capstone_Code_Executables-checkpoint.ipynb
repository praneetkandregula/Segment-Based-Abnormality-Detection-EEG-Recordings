{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1-a8NTZI8QO"
   },
   "source": [
    "# SECTION I\n",
    "\n",
    "Mandatory - this section contains installations, imports and variables that need to set in order to run all sections smoothly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_mbqEFJxI5_5",
    "outputId": "12b81575-2c99-4e55-f070-4ad1817ebc86"
   },
   "outputs": [],
   "source": [
    "# Installations\n",
    "\n",
    "!pip install mne\n",
    "!pip install pyemd\n",
    "!pip install EMD-signal\n",
    "!pip install ewtpy\n",
    "!pip install skfeature-chappers\n",
    "!pip install pdfkit\n",
    "!pip install pyedflib\n",
    "!pip install EDFlib-Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vOkpuYqFgtOl",
    "outputId": "6f631741-4130-4742-f888-1c26213192f5"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8H70n5T-JTBN"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import mne\n",
    "from mne import io\n",
    "import pyedflib\n",
    "import EDFlib\n",
    "import collections\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from PyEMD import EMD, EEMD, CEEMDAN\n",
    "from scipy.stats import skew,kurtosis\n",
    "from scipy.io import loadmat\n",
    "from joblib import Parallel, delayed\n",
    "import time\n",
    "import ewtpy\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer, LabelBinarizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score,accuracy_score, confusion_matrix, precision_score, recall_score,classification_report\n",
    "from sklearn.feature_selection import SelectKBest,chi2, RFE\n",
    "from skfeature.function.similarity_based import fisher_score\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.utils import compute_class_weight\n",
    "\n",
    "import pickle\n",
    "import numpy.ma as ma\n",
    "\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import os\n",
    "from os import path as op\n",
    "import random\n",
    "import statistics\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.under_sampling import OneSidedSelection\n",
    "from matplotlib import pyplot\n",
    "from numpy import where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4TEDodZjJUoW"
   },
   "outputs": [],
   "source": [
    "\n",
    "def clean_up(directory,chosen_model='NONE'):\n",
    "  curr_dir = directory\n",
    "  print(curr_dir)\n",
    "  savFilePaths = [path for path in Path(directory).rglob('*.sav')]\n",
    "  # print(savFilePaths)\n",
    "\n",
    "  for path in savFilePaths:\n",
    "    if chosen_model not in str(path):\n",
    "      # print(chosen_model)\n",
    "      os.remove(path)\n",
    "\n",
    "'''\n",
    "Get EDF file paths from set drive directory\n",
    "'''\n",
    "def get_file_paths(extn, search_dir):\n",
    "  FilePaths = [path for path in Path(search_dir).rglob('*.'+extn)]\n",
    "\n",
    "  return FilePaths\n",
    "\n",
    "'''\n",
    "Extracts the 11 features for segment passed as a numpy array (f)\n",
    "ISSUES : the noverlap and nperseg are removed for now - need to fix\n",
    "'''\n",
    "def feat_extract(f,Fs, welchWin = 1024):\n",
    "    #AM and BM\n",
    "    fhilbert = signal.hilbert(f)#hilbert transform\n",
    "    fhilbert = fhilbert[150:-150]# to avoid border effects\n",
    "    fphase = np.unwrap(np.angle(fhilbert))\n",
    "    A = abs(fhilbert)#instantaneous amplitude\n",
    "    inst_freq = np.diff(fphase)*Fs/(2*np.pi)#instantaneous frequency\n",
    "    E = (np.linalg.norm(fhilbert)**2)/len(fhilbert)\n",
    "    CW = np.sum(np.diff(fphase)*Fs*(A[0:-1]**2))/(2*np.pi*E)\n",
    "    AM = np.sqrt(np.sum((np.diff(A)*Fs)**2))/E # amplitue modulated\n",
    "    BM = np.sqrt(np.sum(((inst_freq-CW)**2)*(A[0:-1]**2))/E)\n",
    "\n",
    "    #spectral features - Welch\n",
    "    # w, Pxx = signal.welch(f, Fs, nperseg = welchWin, noverlap = round(0.85*welchWin) )\n",
    "    w, Pxx = signal.welch(f, Fs)\n",
    "    PxxNorm = Pxx/sum(Pxx)#normalized spectrum\n",
    "    Sent = -sum(PxxNorm*np.log2(PxxNorm))#spectral entropy\n",
    "    Spow = np.mean(Pxx**2)#spectral power\n",
    "    Cent = np.sum(w*PxxNorm) #frequency centroid\n",
    "    Speak = np.max(Pxx) #peak amplitude\n",
    "    Sfreq = w[np.argmax(PxxNorm)]# peak frequency\n",
    "\n",
    "    #skewness, kurtosis\n",
    "    fskew = skew(f)\n",
    "    fkurt = kurtosis(f)\n",
    "\n",
    "    #Hjorth Parameters\n",
    "    dy_f = np.diff(f)\n",
    "    Hmob = np.sqrt(np.var(dy_f)/np.var(f)) # mean signal frequency\n",
    "    Hcomp =  np.sqrt(np.var(np.diff(dy_f))/np.var(dy_f))/Hmob # rate of change in mean signal frequency\n",
    "    \n",
    "    features = [AM,BM,Sent,Spow,Cent,Speak,Sfreq,fskew,fkurt,Hmob, Hcomp]\n",
    "    featLabels = [\"AM\",\"BM\",\"ent\",\"pow\",\"Cent\",\"pk\",\"freq\",\"skew\",\"kurt\",\"Hmob\",\"Hcomp\"]\n",
    "    \n",
    "    return features,featLabels\n",
    "\n",
    "'''\n",
    "Decomposes + Extracts features for segment f\n",
    "SET   : decomposition method in featsTuple\n",
    "'''\n",
    "def decomp_extract(f,Fs,LPcutoff,Nmodes, FFTregLen = 25, gaussSigma = 5,FFTreg = 'gaussian'):\n",
    "    #Transform EEG\n",
    "    ltemp = int(np.ceil(f.size/2)) #to behave the same as matlab's round\n",
    "    fMirr =  np.append(np.flip(f[0:ltemp-1],axis = 0),f)  \n",
    "    fMirr = np.append(fMirr,np.flip(f[-ltemp-1:-1],axis = 0))\n",
    "    f = np.copy(fMirr)\n",
    "\n",
    "    featNames = [\"Group\",\"decTime\"]\n",
    "    featsTuple = {\"EWT\":0}\n",
    "\n",
    "    f = f - np.mean(f)\n",
    "    b, a = signal.butter(4, LPcutoff/ (0.5 * Fs), btype='low', analog=False)\n",
    "    fp = signal.filtfilt(b, a, f)   \n",
    "   \n",
    "    # Get EWT decomposition features\n",
    "    tic = time.time()\n",
    "    ewt,_,_ = ewtpy.EWT1D(fp, N = Nmodes, log = 0,\n",
    "                          detect = \"locmax\", \n",
    "                          completion = 0, \n",
    "                          reg = FFTreg, \n",
    "                          lengthFilter = FFTregLen,\n",
    "                          sigmaFilter = gaussSigma )\n",
    "    toc = time.time()\n",
    "    featsTuple[\"EWT\"]  = toc-tic #execution time (decomposition ) \n",
    "    if Nmodes != ewt.shape[1]:\n",
    "        print(\"\\nCheck number of EWT modes: %s%.3d\"%(item,ri+1))        \n",
    "\n",
    "    #for each mode, extract features\n",
    "    for mi in range(Nmodes):\n",
    "        featOut, labelTemp = feat_extract(ewt[:,mi],Fs, welchWin = 1024)\n",
    "        featsTuple[\"EWT\"]   = np.append(featsTuple[\"EWT\"] ,featOut)    \n",
    "    \n",
    "    return featsTuple\n",
    "\n",
    "'''\n",
    "Gives serialized segments\n",
    "'''\n",
    "def get_serialized_segments(annotated_file_path, seg_length,Fs):\n",
    "  # Read .fif file\n",
    "  raw_da = io.read_raw_edf(annotated_file_path)\n",
    "  info = raw_da.info['ch_names']\n",
    "  # Make epochs\n",
    "  epochs = mne.make_fixed_length_epochs(raw_da, duration=seg_length, preload=False)\n",
    "  epochs_data = epochs.get_data()\n",
    "  print(\"Length of epochs_data = \",len(epochs_data))\n",
    "  print('DROP LOG  :', epochs.drop_log)\n",
    "\n",
    "  # Processing epochs\n",
    "  X = []\n",
    "  epochNumber = 1\n",
    "  for epoch in epochs_data:\n",
    "    print(\"Processing : Epoch # \",epochNumber)\n",
    "    incl = ['EEG C3-P3', 'EEG C4-P4', 'EEG Cz-Pz', 'EEG F3-C3', 'EEG F4-C4', 'EEG F7-T3', 'EEG F8-T4', 'EEG Fp1-F3', 'EEG Fp1-F7', 'EEG Fp2-F4', 'EEG Fp2-F8', 'EEG Fz-Cz', 'EEG P3-O1', 'EEG P4-O2', 'EEG T3-T5', 'EEG T4-T6', 'EEG T5-O1', 'EEG T6-O2']\n",
    "    do_not_take = ['ECG EKG','Photic','EMG','ECG']\n",
    "    channel_idx = mne.pick_channels(info, include=[], exclude = do_not_take)[:18]\n",
    "    feats_per_channel = [ np.array(decomp_extract(epoch[channel],Fs,LPcutoff = 50,Nmodes = 7)['EWT'][1:])  for channel in channel_idx]\n",
    "    # print(len(feats_per_channel[0]))\n",
    "    epoch_ser = np.concatenate(feats_per_channel)\n",
    "    print(epoch_ser.shape)\n",
    "    X.append(epoch_ser)\n",
    "    epochNumber += 1\n",
    "\n",
    "  print(\"X Inputs :\\n\")\n",
    "  x = np.array(X)\n",
    "  x = np.array(x,dtype = \"O\") \n",
    "  print(x[0])\n",
    "  print(x.shape)\n",
    "\n",
    "  return x\n",
    "\n",
    "'''\n",
    "Stores feature mask after doing annoatted_nehaRFE\n",
    "'''\n",
    "def generate_feature_mask(x, y, Nfeats):\n",
    "  # RFE\n",
    "  rfeModel = SVC(kernel=\"linear\", C=0.025,probability = True,gamma = 'scale')\n",
    "  rfeSelect = RFE(rfeModel,n_features_to_select = Nfeats)\n",
    "  rfe_fit = rfeSelect.fit(x, y)\n",
    "  x = x[:,rfe_fit.support_]\n",
    "\n",
    "  # Saving the feature mask\n",
    "  print(\"FEATURE MASK : \",rfe_fit.support_)\n",
    "  with open(post_rfe_dir+'feature_mask.txt', 'w') as f:\n",
    "    for item in rfe_fit.support_:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "  \n",
    "  return x\n",
    "\n",
    "'''\n",
    "Trains model and stores it\n",
    "'''\n",
    "def train_model(x,y,kfolds,num_realizn, nmodes):\n",
    "  # Classifier stuff\n",
    "  K_folds = kfolds\n",
    "  realizations = num_realizn\n",
    "  Nmodes = nmodes\n",
    "  max_accuracy_so_far = 0\n",
    "  names =[\"Linear_SVM\"]\n",
    "  clfAccAll = {}\n",
    "  clfReportAll = {}\n",
    "  for name in names:\n",
    "    clfAccAll.setdefault(name,{})\n",
    "    clfReportAll.setdefault(name,{})\n",
    "\n",
    "  # You can uncomment other classifiers below to test their performances, we have uncommented our best model for now\n",
    "\n",
    "  classifiers = [\n",
    "      # KNeighborsClassifier(3),\n",
    "      SVC(probability=True,C=0.1, gamma =1, kernel = 'linear', class_weight = 'balanced'),\n",
    "      # SVC(probability = True,gamma = 'scale',kernel='rbf'),\n",
    "      # GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "      # MLPClassifier(alpha=1,max_iter = 1000),\n",
    "      # xgb.XGBClassifier()\n",
    "      ]\n",
    "\n",
    "  # Initialize performance variables\n",
    "  AllStats = {}\n",
    "  AllStatsDF = {}\n",
    "  GridSearchStats = {}\n",
    "  AllStatsMean = {}   \n",
    "  AllStatsSTD = {}   \n",
    "\n",
    "  for name in names:\n",
    "      AllStats[name] = {\"Accuracy\":np.zeros([realizations,K_folds]),\n",
    "                        \"Precision Mean\":np.zeros([realizations,K_folds]),\n",
    "                        \"Recall Mean\":np.zeros([realizations,K_folds]),\n",
    "                        \"F1 Score Mean\":np.zeros([realizations,K_folds]),\n",
    "                        \"Specificity Mean\":np.zeros([realizations,K_folds]),\n",
    "                        }   \n",
    "\n",
    "  # Loop over realizations\n",
    "  for i in range(realizations):\n",
    "      print(\"########################## Realization \"+str(i)+\" ##########################\")\n",
    "      skf = StratifiedKFold(n_splits=K_folds,shuffle = True) #5-fold validation\n",
    "\n",
    "      for tupTemp,ki in zip(skf.split(x, y),range(K_folds)):\n",
    "          print(\"------- Fold #\"+str(ki)+\" -------\")\n",
    "          train_idx, test_idx = tupTemp[0],tupTemp[1]\n",
    "          X_train, X_test = x[train_idx], x[test_idx]\n",
    "          y_train, y_test = y[train_idx], y[test_idx]  \n",
    "\n",
    "          # Loop over Classifiers \n",
    "          for name, clf in zip(names, classifiers):   \n",
    "              \n",
    "              # Fit the model\n",
    "              modelFit = clf.fit(X_train, y_train)\n",
    "            \n",
    "              # Predict using model\n",
    "              yPredicted = modelFit.predict(X_test)\n",
    "              probsTest = modelFit.predict_proba(X_test)\n",
    "\n",
    "              # Save model\n",
    "              curr_accuracy = accuracy_score(y_test, yPredicted)\n",
    "              print(name + \" = \",curr_accuracy)\n",
    "              \n",
    "              if curr_accuracy > max_accuracy_so_far:\n",
    "                max_accuracy_so_far = curr_accuracy\n",
    "                # Saving Model - per realization\n",
    "                filename = 'highest_accuracy_model.sav'\n",
    "                pickle.dump(modelFit, open(filename, 'wb'))\n",
    "              \n",
    "              # Store all \n",
    "              filename = name+'_r'+str(i)+'_f'+str(ki)+'_a'+str(curr_accuracy)+ '.sav'\n",
    "              pickle.dump(modelFit, open(model_dir+filename, 'wb'))\n",
    "              \n",
    "              # Add it it to all \n",
    "              clfAccAll[name].setdefault(filename,\"\")\n",
    "              clfAccAll[name][filename] = curr_accuracy\n",
    "              clfReportAll[name][filename] = classification_report(y_test, yPredicted)\n",
    "\n",
    "\n",
    "              # AUC -  #ictal class as positive \n",
    "              if len(np.unique(y)) > 2:\n",
    "                  AUCs = roc_auc_score(LabelBinarizer().fit_transform(y_test), probsTest, average = None)\n",
    "              else:  \n",
    "                  AUCs = roc_auc_score(y_test, probsTest[:,1], average = None)\n",
    "\n",
    "              #Sensitivity and Specificity\n",
    "              cMatrix = confusion_matrix(y_test, yPredicted)\n",
    "              \n",
    "              FP = cMatrix.sum(axis=0) - np.diag(cMatrix)  \n",
    "              FN = cMatrix.sum(axis=1) - np.diag(cMatrix)\n",
    "              TP = np.diag(cMatrix)\n",
    "              TN = cMatrix.sum() - (FP + FN + TP)\n",
    "\n",
    "              # Precsion\n",
    "              P = TP/(TP+FP)\n",
    "\n",
    "              # Recall \n",
    "              R = TP/(TP+FN)\n",
    "\n",
    "              # F1 Score\n",
    "              F1 = 2*(P*R/(P+R))\n",
    "\n",
    "              # Specificity\n",
    "              S = TN/(TN+FP)\n",
    "\n",
    "              # Fill performance variable\n",
    "              AllStats[name][\"Accuracy\"][i,ki] = accuracy_score(y_test, yPredicted)\n",
    "              AllStats[name][\"Precision Mean\"][i,ki] = np.mean(P)\n",
    "              AllStats[name][\"Recall Mean\"][i,ki] = np.mean(R)\n",
    "              AllStats[name][\"F1 Score Mean\"][i,ki] = np.mean(F1)\n",
    "              AllStats[name][\"Specificity Mean\"][i,ki] = np.mean(S)              \n",
    "\n",
    "  AllStatsDF = [0]*len(names)\n",
    "  for idx, name in enumerate(names):    \n",
    "      for istat in AllStats[name].keys():\n",
    "          AllStats[name][istat] = np.mean(AllStats[name][istat],axis = 1)\n",
    "      AllStatsDF[idx] = pd.DataFrame.from_dict(AllStats[name])\n",
    "      AllStatsDF[idx][\"Nmodes\"] = Nmodes\n",
    "      AllStatsDF[idx][\"Classifier\"] = name\n",
    "\n",
    "  # Choosing model closest to the higher average accuracy\n",
    "  print(\"Highest Accuracy : \",max_accuracy_so_far)\n",
    "\n",
    "  return AllStatsDF, clfAccAll, clfReportAll\n",
    "\n",
    "'''\n",
    "Make the X_test using the feature mask and X\n",
    "'''\n",
    "def get_chosen_features(x, directory,feature_mask_file):\n",
    "  # Generate feature mask\n",
    "  map ={'True':True, 'False':False}\n",
    "  with open(directory+feature_mask_file) as f:\n",
    "    feature_mask = np.array([ map[i.strip('\\n').strip(' ')] for i in f.readlines()]) # 1 x 77\n",
    "  \n",
    "  # Use mask to create input\n",
    "  X_input =  x[:,feature_mask]\n",
    "\n",
    "  return X_input\n",
    "\n",
    "def valid(label):\n",
    "  chosen = ['slowing','spike','slowdown','Slowing','ignore']\n",
    "  mlabel = None\n",
    "\n",
    "  if label == 'slowdown':\n",
    "    label = 'Slowing'\n",
    "    \n",
    "  if label in chosen:\n",
    "    if label != 'POST':\n",
    "      mlabel = label.strip(' ').capitalize()\n",
    "    else:\n",
    "      mlabel = 'POSTS'\n",
    "    \n",
    "  return mlabel\n",
    "\n",
    "def get_labels_tuh(f, seg_length):\n",
    "  # Read .edf file\n",
    "  raw_da = io.read_raw_fif(f)\n",
    "  \n",
    "  # Make epochs\n",
    "  epochs = mne.make_fixed_length_epochs(raw_da, duration=seg_length, preload=False)\n",
    "  epochs_data =  epochs.get_data()\n",
    "  print(len(epochs_data))\n",
    "\n",
    "  # Setting up\n",
    "  labels = {}\n",
    "  for i in range(len(epochs_data)):\n",
    "    labels[i] = []\n",
    "  \n",
    "  # Get details\n",
    "  onset = raw_da.annotations.onset\n",
    "  duration = raw_da.annotations.duration\n",
    "  description = raw_da.annotations.description\n",
    "\n",
    "  # Transfer labels\n",
    "  for i in range(len(onset)):\n",
    "    curr_label = valid(description[i])\n",
    "    \n",
    "    if curr_label:\n",
    "      \n",
    "      dur = duration[i]\n",
    "      start = onset[i]\n",
    "      end = start + dur\n",
    "\n",
    "      start_seg = round(start/seg_length)\n",
    "      end_seg = round(end/seg_length)\n",
    "\n",
    "      for seg in range(start_seg,end_seg):\n",
    "        labels[seg].append(curr_label)  \n",
    "\n",
    "      if start_seg == end_seg:\n",
    "        labels[start_seg].append(curr_label)\n",
    "\n",
    "  filtered_labels = {}\n",
    "  for key, value in labels.items():\n",
    "    if key!=0:\n",
    "      if 'Spike' in value and 'Slowing' in value:\n",
    "        value =['Spike']\n",
    "      if 'Ignore' in value :\n",
    "        continue\n",
    "      if value!=[]:\n",
    "        filtered_labels[key] = value[:1]\n",
    "      else:\n",
    "        filtered_labels[key] = ['Normal']\n",
    "  return filtered_labels\n",
    "\n",
    "def extract_segments(Fs, info, epoch_data, filtered):\n",
    "  X = []\n",
    "  # epochNumber = 1\n",
    "  pick = filtered.keys()\n",
    "  for i in pick:\n",
    "    print(\"NEW SEGMENT\")\n",
    "    epoch = epochs_data[i]\n",
    "    # incl = ['EEG C3-P3', 'EEG C4-P4', 'EEG Cz-Pz', 'EEG F3-C3', 'EEG F4-C4', 'EEG F7-T3', 'EEG F8-T4', 'EEG Fp1-F3', 'EEG Fp1-F7', 'EEG Fp2-F4', 'EEG Fp2-F8', 'EEG Fz-Cz', 'EEG P3-O1', 'EEG P4-O2', 'EEG T3-T5', 'EEG T4-T6', 'EEG T5-O1', 'EEG T6-O2']\n",
    "    do_not_take = ['ECG EKG','Photic','EMG','ECG']\n",
    "    channel_idx = mne.pick_channels(info['ch_names'], include=[], exclude = do_not_take)\n",
    "    print(channel_idx)\n",
    "    feats_per_channel = [ np.array(decomp_extract(epoch[ch_num],Fs,LPcutoff = 50,Nmodes = 7)['EWT'][1:])  for ch_num in channel_idx]\n",
    "    epoch_ser = np.concatenate(feats_per_channel)\n",
    "    print(epoch_ser.shape)\n",
    "    filtered[i].append(epoch_ser)\n",
    "\n",
    "  return filtered\n",
    "\n",
    "'''\n",
    "Annotates and displays the EEG based on classifier predictions\n",
    "SET : in fLoad[x], x = data if unknown file | = group if NSC data\n",
    "SET : info is done manually for now\n",
    "'''\n",
    "def annotate_and_plot(pred, raw_file_path, seg_length,fs):\n",
    "  fname = raw_file_path.split('%')[0] + raw_file_path.split('%')[2]\n",
    "  raw = io.read_raw_edf(fname)\n",
    "  \n",
    "  onset = []\n",
    "  duration = []\n",
    "  description = []\n",
    "  my_annot = mne.Annotations(onset,duration,description)\n",
    "  raw.set_annotations(my_annot)\n",
    "\n",
    "  for idx,val in enumerate(pred):\n",
    "    onset.append(idx*seg_length)\n",
    "    duration.append(seg_length)\n",
    "    description.append(val)\n",
    "  \n",
    "  # Add it to mne object\n",
    "  my_annot = mne.Annotations(onset,duration,description)\n",
    "  raw.set_annotations(my_annot)\n",
    "  # Plot\n",
    "  raw.plot(start=0, duration=12)\n",
    "  mne.export.export_raw(raw_file_path,raw,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Z-VgB0EJbK6"
   },
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "\n",
    "search_dir = '/Dataset/'\n",
    "raw_dir = '/Raw_EEG/'\n",
    "extracted_data_dir = '/New_Extracted_Data/'\n",
    "model_dir = '/New_Trained_Model/'\n",
    "post_rfe_dir = '/New_Post_RFE/'\n",
    "n_features = 20*18\n",
    "kfolds = 10\n",
    "realizations = 10\n",
    "nmodes = 7\n",
    "segment_length = 1\n",
    "feature_mask_file = 'feature_mask.txt'\n",
    "presaved_extracted_data_dir = '/Extracted_Data/'\n",
    "presaved_model_dir = '/Trained_Model/'\n",
    "presaved_post_rfe_dir = '/Post_RFE/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aAj1cXD4JWol"
   },
   "source": [
    "# SECTION II.\n",
    "This section conatins the code for from scratch model training and annotation. This section may take some time to execute.\n",
    "\n",
    "For annotating a file directly move to Section III.\n",
    "\n",
    "EXTRACTION OF SEGMENTS\n",
    "\n",
    "> Each EEG file is broken down into segments. Channel wise signal decomposition and feature extraction is done to create a numerical representation for segment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PAyqqG1MJWLs"
   },
   "outputs": [],
   "source": [
    "# Looping over /data\n",
    "X = []\n",
    "Y = []\n",
    "file_paths = [path for path in Path(search_dir).rglob('*.fif')]\n",
    "file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GvRUrWELamcj"
   },
   "outputs": [],
   "source": [
    "for f in file_paths:\n",
    "  # Load file\n",
    "  raw = io.read_raw_fif(f)\n",
    "  Fs = raw.info['sfreq']\n",
    "  info = raw.info\n",
    "\n",
    "  # Get labels\n",
    "  filtered_labels = get_labels_tuh(f, 1)\n",
    "\n",
    "  # Extract Segments\n",
    "  epochs = mne.make_fixed_length_epochs(raw, duration=1, preload=False)\n",
    "  epochs_data = epochs.get_data()\n",
    "\n",
    "  extracted = extract_segments(Fs,info, epochs_data, filtered_labels)\n",
    "\n",
    "  # Saving extracted segments\n",
    "  data = np.array(list(extracted.values()))\n",
    "  string_f=str(f)\n",
    "  outfile = string_f.split('/')[-1].replace('.','_')\n",
    "  np.save(extracted_data_dir+outfile, data ,allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qiLfT7zAT3tV"
   },
   "outputs": [],
   "source": [
    "# Get all npy files\n",
    "npy_file_paths = [path for path in Path(extracted_data_dir).rglob('*.npy')]\n",
    "# npy_file_paths\n",
    "X = list()\n",
    "Y = list()\n",
    "\n",
    "for fnpy in npy_file_paths:\n",
    "  print(fnpy)\n",
    "  # Load file\n",
    "  ldata = np.load(fnpy, allow_pickle=True)\n",
    "  # Get x and y\n",
    "  x = [d[1] for d in ldata]\n",
    "  y = [d[0] for d in ldata]\n",
    "  # Append to X\n",
    "  X.append(np.array(x))\n",
    "  Y.append(np.array(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9AbsEulqKXRK"
   },
   "outputs": [],
   "source": [
    "# Transforming X and Y to the right form\n",
    "X_inp = np.array(X)\n",
    "X_inp = np.concatenate(X_inp)\n",
    "X_inp = np.array(X_inp,dtype = \"O\")\n",
    "X_inp = StandardScaler().fit_transform(X_inp)\n",
    "X_inp = np.nan_to_num(X_inp,nan=0.00065, posinf=None, neginf=None)\n",
    "\n",
    "Y_inp = np.array(Y)\n",
    "Y_inp = np.concatenate(Y_inp)\n",
    "print(\"Samples per class:\\n\",Counter(Y_inp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KOzEZe-KNms"
   },
   "source": [
    "UNDERSAMPLING\n",
    "\n",
    "> Since the number of normal segmnets are far greater in number than the other classes, we have decided to use OSS to undersample majority class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eKLR-ix4Kelq"
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "X_under, y_under = X_inp, Y_inp\n",
    "for i in range(16):\n",
    "  undersample = OneSidedSelection(n_neighbors=1, n_seeds_S=200,sampling_strategy='majority')\n",
    "  X_under, y_under = undersample.fit_resample(X_under, y_under)\n",
    "\n",
    "# Check the counts\n",
    "print(\"Before\")\n",
    "pp.pprint(Counter(Y_inp))\n",
    "print(\"After\")\n",
    "pp.pprint(Counter(y_under))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NkCrR4moLPPv"
   },
   "source": [
    "FEATURE ELIMINATION\n",
    "\n",
    "> Total number of features is given by 18 x 7 x 11. RFE reduces this to 20*18 features\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q1SiIQxlLOWk"
   },
   "outputs": [],
   "source": [
    "x_rfe = generate_feature_mask(X_under, y_under, n_features)\n",
    "\n",
    "# Save X \n",
    "def save_as_npy(x, outFile):\n",
    "  np.save(outFile, x ,allow_pickle=True)\n",
    "\n",
    "# Get masked X and Y values\n",
    "save_as_npy(x_rfe,post_rfe_dir+'x_rfe')\n",
    "save_as_npy(y_under,post_rfe_dir+'y_rfe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcr0oZThLTJH"
   },
   "source": [
    "TRAINING\n",
    "\n",
    "\n",
    "> Train the model over the extracted data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gPIhSK7gLWGt"
   },
   "outputs": [],
   "source": [
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "AllStatsDF, clfAcc, clfReport = train_model(x_rfe, y_under, kfolds, realizations, nmodes)\n",
    "\n",
    "# Choose a model\n",
    "model = \"Linear_SVM\"\n",
    "df = AllStatsDF[0]\n",
    "max_acc = df['Accuracy'].max()\n",
    "\n",
    "# Get model name with closest accuracy\n",
    "curr_model = clfAcc[\"Linear_SVM\"]\n",
    "od = collections.OrderedDict(sorted(curr_model.items(), key=lambda x : abs(max_acc - x[1])))\n",
    "chosen_model = list(od.items())[0][0]\n",
    "print(\"CHOSEN : \",chosen_model)\n",
    "\n",
    "#Clean up\n",
    "clean_up(model_dir,chosen_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ml-FceURLXiW"
   },
   "source": [
    "\n",
    "ANNOTATING A NEW .edf FILE\n",
    "\n",
    "Load segments from an EDF file and use the chosen model from training phase to annotate the EDF file\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EUmb0vQOVLLi"
   },
   "outputs": [],
   "source": [
    "# Load extracted segments\n",
    "f_edf = '14.edf'\n",
    "f_np = '14_edf-annotated_raw_fif.npy'\n",
    "extracted_features = np.load(extracted_data_dir+f_np,allow_pickle=True)\n",
    "x_patient = np.array([d[1] for d in extracted_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "97QnJHMJWaIt"
   },
   "outputs": [],
   "source": [
    "# Transform X\n",
    "\n",
    "X_inp = np.array(x_patient,dtype = \"O\")\n",
    "X_inp = StandardScaler().fit_transform(X_inp)\n",
    "X_inp = np.nan_to_num(X_inp,nan=0.00065, posinf=None, neginf=None)\n",
    "X_test = get_chosen_features(X_inp,post_rfe_dir, feature_mask_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "icgw6ML5WcFQ"
   },
   "outputs": [],
   "source": [
    "# Load Model and predict\n",
    "\n",
    "model = pickle.load(open(model_dir+chosen_model, 'rb'))\n",
    "yPredicted = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JbZVsKILWlK0"
   },
   "outputs": [],
   "source": [
    "# Annotate and Plot\n",
    "\n",
    "annotate_and_plot(yPredicted,raw_dir+f_edf, segment_length, Fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKbSIxMaduOU"
   },
   "source": [
    "# SECTION III.\n",
    "\n",
    "This section uses a pre-saved model and extracted features from the parent folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WfV2qfzTdsEY"
   },
   "outputs": [],
   "source": [
    "# Load extracted segments\n",
    "f_edf = '14.edf'\n",
    "f_np = '14_edf-annotated_raw_fif.npy'\n",
    "extracted_features = np.load(presaved_extracted_data_dir+f_np,allow_pickle=True)\n",
    "x_patient = np.array([d[1] for d in extracted_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ek0RTslefuyu"
   },
   "outputs": [],
   "source": [
    "# Transform X\n",
    "\n",
    "X_inp = np.array(x_patient,dtype = \"O\")\n",
    "X_inp = StandardScaler().fit_transform(X_inp)\n",
    "X_inp = np.nan_to_num(X_inp,nan=0.00065, posinf=None, neginf=None)\n",
    "X_test = get_chosen_features(X_inp, presaved_post_rfe_dir, feature_mask_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "37lEDFRVfzE2"
   },
   "outputs": [],
   "source": [
    "# Load Model and predict\n",
    "\n",
    "model = pickle.load(open(presaved_model_dir+chosen_model, 'rb'))\n",
    "yPredicted = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 747
    },
    "id": "XS4yj1W6f0hM",
    "outputId": "c31e85ea-3677-4233-801a-65250a2fbc09"
   },
   "outputs": [],
   "source": [
    "# Annotate and Plot\n",
    "# The annotated plot will be saved in the Raw_EEG folder\n",
    "annotate_and_plot(yPredicted,raw_dir+'%annotated%'+f_edf, segment_length, Fs)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Capstone Code Executables.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
